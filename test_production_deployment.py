"""
Direct model testing without HTTP server - production-ready deployment test
"""
import sys
sys.path.append('.')
from backend.models.detector import HateSpeechDetector
import time
import json

print("\n" + "="*80)
print("üéØ PRODUCTION DEPLOYMENT TEST - WORLD-CLASS MODEL (99.39% Accuracy)")
print("="*80 + "\n")

# Initialize detector
print("Initializing hate speech detector...")
start_init = time.time()
detector = HateSpeechDetector()
init_time = (time.time() - start_init) * 1000
print(f"‚úÖ Model loaded in {init_time:.1f}ms\n")

# Comprehensive test suite
test_suite = {
    "Normal Speech": [
        "You are doing an amazing job!",
        "Have a great day everyone!",
        "Let's work together on this project",
        "Thank you for your support",
        "The weather is beautiful today",
        "Mathematics is a fascinating subject",
        "I really appreciate your help",
        "This is a wonderful presentation",
        "Great teamwork everyone!",
        "I'm excited about this opportunity"
    ],
    "Clear Hate Speech": [
        "I hate you and wish you were dead",
        "You're worthless and should die",
        "Your race is subhuman trash",
        "Kill yourself you pathetic loser",
        "All members of that religion are evil",
        "You deserve to be tortured and killed",
        "Die you worthless scum"
    ],
    "Discrimination": [
        "Women belong in the kitchen",
        "Gay people are mentally ill",
        "Old people are useless burdens",
        "Fat people disgust me",
        "Poor people are lazy trash",
        "Disabled people are a burden on society",
        "Transgender people are sick freaks"
    ],
    "Constructive Criticism (Should NOT flag)": [
        "Your work needs improvement",
        "I don't agree with your opinion",
        "That was a terrible decision",
        "This code has some bugs",
        "Your presentation could be better"
    ],
    "Edge Cases": [
        "I hate traffic jams",  # Should NOT flag - hating objects/situations
        "This weather is killing me",  # Should NOT flag - figure of speech
        "I'm dying of laughter",  # Should NOT flag - expression
        "You're killing it!",  # Should NOT flag - compliment
        "That test killed me"  # Should NOT flag - expression
    ]
}

all_results = []
total_tests = sum(len(texts) for texts in test_suite.values())

print(f"Running {total_tests} comprehensive tests...\n")
print("="*80 + "\n")

for category, texts in test_suite.items():
    print(f"üìÇ {category}")
    print("-" * 80)
    
    correct = 0
    total = len(texts)
    times = []
    
    for text in texts:
        start = time.time()
        result = detector.analyze(text)
        elapsed = (time.time() - start) * 1000
        times.append(elapsed)
        
        # Determine expected result
        if category in ["Normal Speech", "Constructive Criticism (Should NOT flag)", "Edge Cases"]:
            expected_hate = False
        else:
            expected_hate = True
        
        is_correct = (result['is_hate_speech'] == expected_hate)
        if is_correct:
            correct += 1
        
        # Icon based on result
        if is_correct:
            icon = "‚úÖ"
        else:
            icon = "‚ùå"
        
        # Display result
        prediction = "Hate Speech" if result['is_hate_speech'] else "Normal"
        expected = "Hate Speech" if expected_hate else "Normal"
        
        print(f"{icon} {text[:55]:55} ‚Üí {prediction:12} (conf: {result['confidence']:.3f}, {elapsed:.1f}ms)")
        
        all_results.append({
            'category': category,
            'text': text,
            'expected': expected,
            'prediction': prediction,
            'confidence': result['confidence'],
            'is_correct': is_correct,
            'time_ms': elapsed,
            'hate_category': result['category']
        })
    
    avg_time = sum(times) / len(times) if times else 0
    accuracy = (correct / total * 100) if total > 0 else 0
    print(f"\n   Category Accuracy: {correct}/{total} ({accuracy:.1f}%)")
    print(f"   Average Response Time: {avg_time:.1f}ms\n")

# Overall statistics
print("\n" + "="*80)
print("üìä OVERALL RESULTS")
print("="*80 + "\n")

total_correct = sum(1 for r in all_results if r['is_correct'])
overall_accuracy = (total_correct / len(all_results) * 100) if all_results else 0

avg_confidence = sum(r['confidence'] for r in all_results) / len(all_results) if all_results else 0
avg_time = sum(r['time_ms'] for r in all_results) / len(all_results) if all_results else 0
max_time = max(r['time_ms'] for r in all_results) if all_results else 0
min_time = min(r['time_ms'] for r in all_results) if all_results else 0

print(f"Total Tests: {len(all_results)}")
print(f"Correct Predictions: {total_correct}")
print(f"Overall Accuracy: {overall_accuracy:.1f}%")
print(f"\nPerformance Metrics:")
print(f"  Average Confidence: {avg_confidence:.3f}")
print(f"  Average Response Time: {avg_time:.1f}ms")
print(f"  Max Response Time: {max_time:.1f}ms")
print(f"  Min Response Time: {min_time:.1f}ms")
print(f"  Throughput: {1000/avg_time:.1f} predictions/second")

# Analyze failures
failures = [r for r in all_results if not r['is_correct']]
if failures:
    print(f"\n‚ö†Ô∏è  Failed Cases ({len(failures)}):")
    print("-" * 80)
    for f in failures:
        print(f"  Category: {f['category']}")
        print(f"  Text: {f['text']}")
        print(f"  Expected: {f['expected']}, Got: {f['prediction']} (confidence: {f['confidence']:.3f})")
        print()
else:
    print(f"\nüéâ ALL TESTS PASSED - 100% ACCURACY!")

# Production readiness assessment
print("\n" + "="*80)
print("üöÄ PRODUCTION READINESS ASSESSMENT")
print("="*80 + "\n")

# Criteria for production deployment
criteria = {
    "Accuracy ‚â• 95%": overall_accuracy >= 95,
    "Average response time < 100ms": avg_time < 100,
    "Max response time < 500ms": max_time < 500,
    "Model loaded successfully": detector.model_loaded,
    "Throughput > 10 requests/sec": (1000/avg_time) > 10
}

print("Deployment Criteria:")
for criterion, passed in criteria.items():
    status = "‚úÖ PASS" if passed else "‚ùå FAIL"
    print(f"  {status} - {criterion}")

all_passed = all(criteria.values())
print()
if all_passed:
    print("üéâ SYSTEM IS PRODUCTION-READY!")
    print("   The model meets all deployment criteria and can be deployed to production.")
else:
    print("‚ö†Ô∏è  System needs optimization before production deployment.")

# Real-world projections
print(f"\nüìà Real-World Performance Projections:")
print(f"   With {overall_accuracy:.1f}% accuracy:")
print(f"   - Out of 10,000 messages:")
print(f"     ‚Ä¢ ~{int(overall_accuracy * 50)} hate messages correctly detected (out of 5,000)")
print(f"     ‚Ä¢ ~{int(overall_accuracy * 50)} normal messages correctly identified (out of 5,000)")
print(f"     ‚Ä¢ ~{int((100-overall_accuracy) * 50)} messages misclassified")
print(f"   - Processing capacity: {int(1000/avg_time * 3600)} messages per hour")
print(f"   - Daily capacity (24/7): {int(1000/avg_time * 86400):,} messages")

print("\n" + "="*80)
print("‚úÖ DEPLOYMENT TEST COMPLETE")
print("="*80 + "\n")

# Save results to file
results_file = "deployment_test_results.json"
with open(results_file, 'w', encoding='utf-8') as f:
    json.dump({
        'summary': {
            'total_tests': len(all_results),
            'correct': total_correct,
            'accuracy': overall_accuracy,
            'avg_confidence': avg_confidence,
            'avg_time_ms': avg_time,
            'max_time_ms': max_time,
            'min_time_ms': min_time,
            'throughput_per_sec': 1000/avg_time,
            'production_ready': all_passed
        },
        'criteria': criteria,
        'detailed_results': all_results
    }, f, indent=2, ensure_ascii=False)

print(f"üìÑ Detailed results saved to: {results_file}")
